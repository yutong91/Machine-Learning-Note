### 决策树
##### 基本原理
表示基于特征对实例进行分类的过程
* 可以认为是if-then规则的集合
* 也可以认为是定义在特征空间与类空间上的条件概率分布

##### 优点：具有可读性，分类速度快
##### 三个步骤：
- 特征选择
- 决策树的生成
- 决策树的修剪

##### 定义
分类决策树模型是一种描述对实例进行分类的树形结构，决策树由结点（node）和有向边（directed edge）组成。
结点具有两种类型：
- 内部结点(internal node):表示一个特征或者属性
- 叶结点(leaf node):表示一个类

##### if-then规则
**性质：互斥并且完备**
- `每一个实例都被一条路径或者一条规则所覆盖，而且只被一条路经或一条规则所覆盖。`
- `这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。`

##### 条件概率分布
1. 决策树表示`给定特征条件`下类的条件概率分布
2. 这一条件概率分布定义在特征空间的一个划分(partition)上 -> 将特征空间划分为互不相交的单元(cell)或者区域(region) -> 在每个单元定义一个类的概率分布就构成了一个条件概率分布
3. 决策树的一条路经对应与划分中的一个单元。
4. 假设X为表示特征的随机变量，Y为表示类的随机变量，那么这个条件概率分布可以表示为P(Y|X)。X取值于给定划分下单元的集合，Y取决于类的集合。
5. 各叶结点（单元）上得条件概率往往偏向于一个类，即属于某一类的概率较大。-> 决策树分类时将该结点的实例强行分到条件概率大的那一类去。

##### 决策树学习
1. 本质：从训练数据集中归纳出一组分类规则。
2. 损失函数：通常是正则化的最大似然函数。
3. 学习策略：以损失函数为目标函数的最小化
4. 确定损失函数后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题 -> 现实中决策树学习算法通常采用`启发式`学习，近似求解，得到的决策树是次最优的。
5. 不断地递归，得到的决策树，对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，可能发生**过拟合**。
6. 所以，需要对决策树自下而上进行剪枝，将树变得更简单，从未具有更好的泛化能力。-> 去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。

#### 特征选择
1. 如果利用一个特征进行分类的结果与随机分类的结果没有太大差别，则称这个特征是没有分类能力的。
2. 通常特征选择的准则是信息增益或信息增益比。

##### 信息增益
1. 熵（entropy）：表示随机变量不确定性的度量
2. 设X是一个取有限个值的离散随机变量，其概率分布为：P(X=xi) = pi, i=1,2,...,n
  则随机变量X的熵定义为：
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br /> <br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br /> 

#### 决策树生成
